# [Google Research Football with Manchester City F.C.](https://www.kaggle.com/c/google-football)

## 期間
- 2020/09 - 2020/12

## Overall
- Google Researchが開発したサッカーのシミュレーション環境（3,000フレーム）で参加者間でagentを作成し、対戦（相手はkaggleで自動選択）。勝敗によるpointで順位が決まるコンペティション。
- サッカーのシミュレーション環境を使用するため、データはないが、自チーム、他チームの行動ログは取得可能。Public, Privateはなく、締め切り後2週間、試合を続けた後に最終順位が確定する。
- 評価指標、勝敗によってμ（初期値:600）が変動していく。
- 1,138 teams

## 1st place solution
- ディスカッション：https://www.kaggle.com/c/google-football/discussion/202232
- ソースコード：なし
- RLとselfplayを使用し、agentを構築した。
- [こちら](https://arxiv.org/abs/1912.09729)に似たフレームワークを使用。
  - SMMとCNNブロックは学習のスピードが遅く、メモリ消費量が多いため廃止した。
  - 価値推定の分散を減らすため[multi-head value(MHV)](https://arxiv.org/abs/2011.12692)を使用。報酬はさまざまなヘッドに分解され、さまざまな割引係数で累積される。背後にある考えはインターセプト、オフサイド、スライディングのように直近のアクションとのみ高い相関があるものと、ゴールのように長い期間でのアクションの累積で達成されるものがあることから。価値推定はヘッドの値の加重和となる。
  - 標準の115次元を拡張。
    - チームメイトと対戦相手の間の相対的な位置、方向
    - アクティブなプレイヤーとボールの間の相対的な位置
    - 潜在的なオフサイドチームメイトをマークするオフサイドフラグ
  - 報酬がスパースなため、（成功、失敗した）アクション（パスカット、オフサイド、スライディング、有効なパス、ゴール）に報酬をつけた。
  - [GAIL](https://proceedings.neurips.cc/paper/2016/file/cc7e2b878868cbae992d1fb743995d8f-Paper.pdf)を利用して他のチームから学んだ(他チームの戦術を直接報酬として実装するのが難しかったため)。GAILを利用してモデルをトレーニングし、他チームをリプレイ。これは学習を進める際に固定された対戦相手としても使え、モデルのロバストさを向上させた。
  - 戦略に多様性とロバストさを追加するために[リーグトレーニング](https://www.nature.com/articles/s41586-019-1724-z)を採用した。
  - 評価は内部LBを作成した（Public LBと高い相関あり）・


## 2nd place solution
- ディスカッション：https://www.kaggle.com/c/google-football/discussion/202977
- ソースコード：なし
- RLEaseのdistributed IMPALAを使用した。
  - 最初にeasy botで学習し、次にhard botで収束するまで学習(PVE)。その後にselfplayを行った。
  - simple115 wrapperを拡張。
  - agentのActionでは、ボールを持っているプレイヤーのスライディング、ボールを持っていないプレイヤーのパスなどを無効化し、探索スペースを削減。しかし、最後の一週間で1st teamの試合を見るとパサーからボールを受ける選手がすでに適切なポジションと向きを選択していることに気づき、制約を解除した（これによって若干スコアが改善した）。
  - RewardはRewardとcheckpointを使用。スパースなRewardに比べて、checkpointは役に立ったが決して強いagentを作るわけではなかった。あくまでagentに攻撃の手がかりを与える程度。
  - 優れたPVEモデルが出来たら、selfplayで学習。ここではRewardがそこまでスパースにならなかった。
  - ボールを奪う、パスを通すことにも報酬を作ったが強いagentには至らなかった。この理由は作った各報酬に適切な値をつけることが難しいという問題と認識。
  - 各実験には1,000コアのCPUを使用。最後の2週間は2,500コアを使用した。
  - カリキュラム学習の使用（フリーキックからのロングシュートを習得）、他に1st teamの動作をクローンして対戦、


## 3rd place solution
- ディスカッション：https://www.kaggle.com/c/google-football/discussion/200709
- ソースコード：なし
- ルールベースから入ったが立ち行かないことに気づき、[こちら](https://www.nature.com/articles/s41586-019-1724-z?proof=trueMay)に似たアプローチをとった。
  - 教師ありで1st teamのagentのアクションを確率予測するネットワークを学習、それを初期値にRL。元のポリシーを忘れないように元の結果とのKL distribution lossを追加（学習が安定、agentに多様性を追加するのに役立った）。
  - 学習が遅くなってきたら次のラウンドの開始点として最終的なcheckpointを選択して、それを新しい参照ポリシーにして学習する。
  - 観測にマルコフ性を持たせるために、最後のパス、シュート、スライディングからの時間をencodingとして追加。
  - CNNとtransformersを試したが、速度に優れたCNNを採用。runtimeに気をつけながら諸々の特徴追加やアーキテクチャの改良を行った。
  - Public LB上ではスコアの分散が大きすぎて、モデルを評価しにくかった（同じagentを1日後に提出するとスコアが90近く違うことも）。したがって、自作ルールベースagent、公開agent、クローンagent、RLエージェントを使って内部LBを作り、ここで評価した。
  - RLはIMPALAをpytorchで実装
  - 自作の報酬は機能しなかった。単純な報酬にしたところRLが機能し始めた。そしてTD学習がうまく機能した。
  - 実験の半分はselfplayであり、selfplayが機能した。selfplayは2倍のデータ効率であり、現在のポリシーの資格を検出できる。一方でselfplayのみだとサイクルに陥る可能性がある。残りの半分は固定された対戦相手との試合（約10team）。これらの組み合わせを数サイクルすると目立った改善が見られた。
